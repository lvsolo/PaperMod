---
title: "Focal Loss for Binary or Multi Classes"
author: "lvsolo"
math: true
date: "2024-10-14"
tags: ["DeepLearning", "Loss", "training", "YOLO", "Experiments"]
---

# Focal Loss理解
Focal Loss本身是针对困难样本进行权重倾斜的一种方法，他的实现需要通过根不同的具体的loss函数结合进行，当与不同loss结合时，会有不同的表现形式。

# Cross Entropy Loss
## Entropy

## Binary Cross Entropy 
## Multi-classes Cross Entropy 
在多分类模型中，交叉熵损失（Cross-Entropy Loss）衡量模型预测的类别分布与实际类别之间的差异。其计算过程主要关注的是正确分类的概率，而错误分类的预测值（非目标类别的概率）对损失的直接影响相对较小。原因可以从交叉熵损失的定义和计算方式来理解：

### 1. 交叉熵损失的定义
对于一个多分类问题，假设模型输出的概率分布为 \(\mathbf{p}\)，真实标签用 one-hot 编码表示为 \(\mathbf{q}\)。对单个样本的交叉熵损失公式为：

$$ CrossEntropy = - \sum_{i} q_i \log(p_i) $$

其中：
 <!-- \$\$ q_i $$ 是真实标签的 one-hot 编码值，目标类别对应的值为 1，其他类别为 0。 -->
 <!-- $$ p_i $$ 是模型对类别 $ i $ 的预测概率。 -->


<!-- \\( q_i \\) 是真实标签的 one-hot 编码值，目标类别对应的值为 1，其他类别为 0。 -->

公式对于二分类问题：



如果我们不考虑多分类预测值是通过 **softmax** 得到的结果，那么确实可以借鉴二分类交叉熵的思路，显式地为每个错误预测类别添加损失。标准的多分类交叉熵损失只考虑了正确类别的预测概率，而忽略了错误类别的预测。如果不用 softmax，并希望错误类别的预测也对损失产生影响，可以借鉴二分类交叉熵的方式进行修改。

以下是具体的思路和公式，帮助你生成一篇完整的 markdown 博文：

---

## 多分类交叉熵损失中的错误预测

在标准的多分类交叉熵中，损失函数仅考虑了正确类别的预测概率，而没有直接处理错误类别的预测值。这是因为我们通常使用 softmax 函数将模型的输出转化为概率分布，错误类别的概率间接影响了正确类别的概率。

### 标准的多分类交叉熵损失

假设：
- \( q_i \) 是真实标签的 one-hot 编码（正确类别为 1，其他类别为 0）。
- \( p_i \) 是通过 softmax 得到的模型对类别 \(i\) 的预测概率。

标准的多分类交叉熵损失为：

\[
L = - \sum_{i} q_i \log(p_i)
\]

这里，只有正确类别对应的 \(q_i = 1\) 会对损失产生影响，其他错误类别的预测概率 \(p_i\) 对损失的直接贡献为 0，因为对应的 \(q_i = 0\)。

### 二分类交叉熵的思路

在二分类问题中，交叉熵损失函数同时考虑了正类和负类的预测损失。公式如下：

\[
L(y, \hat{y}) = - \left[ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right]
\]

其中，\((1 - y) \log(1 - \hat{y})\) 是对负类（错误分类）的惩罚。

### 如何将此思路扩展到多分类

如果不使用 softmax，而是直接使用模型输出的 logits（未归一化的得分），我们可以借鉴二分类交叉熵的思路，为每个错误类别添加惩罚。具体的损失函数修改为：

\[
L = - \left[ q_{i^*} \log(p_{i^*}) + \sum_{i \neq i^*} (1 - q_i) \log(1 - p_i) \right]
\]

其中：
- \( i^* \) 是真实类别。
- \( p_i \) 是模型对类别 \( i \) 的预测值，通常可以直接是 logits，或者通过 Sigmoid 函数将 logits 转化为概率。

### 公式解释

- **第一项** \( q_{i^*} \log(p_{i^*}) \)：与标准的交叉熵相同，计算的是正确类别的预测损失。
- **第二项** \( \sum_{i \neq i^*} (1 - q_i) \log(1 - p_i) \)：显式地加入了对错误类别的惩罚，如果模型对错误类别的预测值 \( p_i \) 较高，则 \( \log(1 - p_i) \) 会较大，从而增加损失。

### 注意事项

1. **预测值的问题**：
   如果不使用 softmax，而是直接使用 logits，可能会出现 \( p_i \) 不在 \([0, 1]\) 范围内的情况。因此，可以对每个 \( p_i \) 使用 **Sigmoid 函数** 来将 logits 转化为概率：

   \[
   p_i = \sigma(z_i) = \frac{1}{1 + e^{-z_i}}
   \]

   其中 \( z_i \) 是模型对类别 \( i \) 的原始输出（logits）。

2. **类别之间的独立性问题**：
   在多分类问题中，使用 softmax 可以保证类别概率总和为 1。如果分别对每个类别使用 Sigmoid，则每个类别的预测将相互独立，无法保证总和为 1。这在某些情况下可能不符合问题的要求，需要根据具体任务来选择。

### 总结

在不考虑 softmax 的情况下，我们可以借鉴二分类交叉熵的思路，通过修改损失函数显式地加入对错误预测类别的惩罚项。这种方法的优点是可以更直接地控制错误分类对损失的影响，但同时需要处理 logits 的归一化问题，避免出现不可解释的概率分布。

---

这篇文章详细介绍了在不使用 softmax 的多分类交叉熵中如何加上错误分类的损失，欢迎尝试并根据任务需求调整损失函数。
